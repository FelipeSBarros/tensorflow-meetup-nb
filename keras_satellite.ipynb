{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo_dymaxion](img/dymaxion_labs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de imágenes satelitales con CNNs usando Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a presentar una posible aplicación de redes convolucionales (CNN) para imágenes satelitales. En este caso, buscaremos detectar asentamientos precarios usando solamente esa fuente de datos. De esta manera, podemos actualizar periódicamente la cantindad de km2 de asentamientos y así tener un indicador que pueda anticiparse a las estadísticas de hábitat.\n",
    "\n",
    "Luego vamos a mostrar un ejemplo usando segmentación.\n",
    "\n",
    "Para más información pueden ver nuestro sitio [www.dymaxionlabs.com](http://www.dymaxionlabs.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini introducción a imágenes satelitales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen distintos tipos de imágenes satelitales (ópticas), una clasificación que se puede hacer es de acuerdo a su resolución espacial.\n",
    "\n",
    "Podemos distinguir entre (no exhaustivo):\n",
    "\n",
    "* **Resolución baja**: [MODIS](https://modis.gsfc.nasa.gov/), 250-1000m por pixel.\n",
    "    * Utilizadas para investigación en cambio climático, recursos naturales y análisis de grandes superficies.\n",
    "    * Imagen diaria.\n",
    "    * Gratuitas, disponibles en varias fuentes. Por ejemplo [Google Earth Engine](https://earthengine.google.com/)\n",
    "    * Casos de uso: predicción de cosecha para el agro para grandes supericies. Indices de calidad del aire.\n",
    "\n",
    "\n",
    "* **Resolución media**: Landsat (30m), Sentinel-2 (10m), RapidEye (5m).\n",
    "    * Utilizadas para análisis urbano y agro.\n",
    "    * Cada 15 días (Landsat) y 5 días (Sentinel-2).\n",
    "    * Landsat y Sentinel-2 son gratuitas, disponibles en varias fuentes.\n",
    "    * Casos de uso: predicción de cosecha e indicadores relacionados con agricultura de precisión.\n",
    "\n",
    "\n",
    "* **Resolución alta**: WorldView-4 (0.3m de resolución)\n",
    "    * utilizadas para detección de objetos y monitoreo de gran precisión.\n",
    "    * Revisita variable según la zona.\n",
    "    * No son gratuitas (~24 USD/km2).\n",
    "    * Casos de uso: conteo de autos en parking lots de supermercados y contenedores en puertos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Museo Louvre (Paris, Francia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentinel-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![louvre_sen](img/louvre_sen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WorldView-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![louvre_wv](img/louvre_wv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neuronales y Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tradicionalmente se utilizaron métodos cómo clasificación por pixel o Análisis de Imágenes basada en objectos (OBIA). Un enfoque que que se hizo popular recientemente es el de usar redes neuronales convolucionales para hacer la clasificación de imágenes. Se combina con técnicas de *computer vision* para filtrar y segmentar las imágenes de acuerdo a la clase que se busca detectar.\n",
    "\n",
    "A continuación va un ejemplo de cómo procesar imáges de un territorio para clasificar asentamientos precarios usando redes neuronales convolucionales. Vamos a hacer transfer learning, aprovechando de redes ya entrenadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las imágenes tal cual son capturadas por los sensores de los satélites deben ser preprocesadas para que puedan ser utilizados. El proceso tiene varias etapas:\n",
    "\n",
    "* **Corrección radiométrica**\n",
    "* **Ortorrectificación**\n",
    "* **Corrección atmosférica**\n",
    "\n",
    "No vamos a entrar en detalle acá porque llevaría mucho tiempo y se va del scope de esta charla :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se particiona la imagen entera en tiles de `n`x`m` pixeles y se cruza con las localizaciones de asentamientos precarios para establecer cuales son True y cuales False. Las dimensiones van de acuerdo a la red que usemos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos de True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![guatemala_vya_t](img/guatemala_vya_t.jpg) ![guatemala_vya_t_2](img/guatemala_vya_t_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos de False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![guatemala_no_vya_f](img/guatemala_no_vya_f.jpg) ![guatemala_no_vya_f_2](img/guatemala_no_vya_f_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada la estructura de los directorios,\n",
    "\n",
    "```\n",
    "data/\n",
    "  train/\n",
    "    true/\n",
    "    false/\n",
    "  validation/\n",
    "    true/\n",
    "    false/\n",
    "```\n",
    "\n",
    "levantamos los datos con una estructura que permita trabajar con Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = glob(os.path.join('data/', 'train/**/', '*.jpg'))\n",
    "validation_files = glob(os.path.join('data/', 'validation/**/', '*.jpg'))\n",
    "\n",
    "img_width, img_height = 256, 256\n",
    "\n",
    "train_data_dir = \"data/train/\"\n",
    "validation_data_dir = \"data/validation/\"\n",
    "\n",
    "nb_train_samples = len(train_files)\n",
    "nb_validation_samples = len(validation_files)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializamos una red ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![paper_resnet](img/paper_resnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94633984/94653016 [============================>.] - ETA: 0ss8"
     ]
    }
   ],
   "source": [
    "model = ResNet50(\n",
    "    weights = \"imagenet\",\n",
    "    include_top = False,\n",
    "    input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien la arquitectura de esta red es más profunda que el caso de VGG16 y VGG19, el hecho de que use _global average pooling_ en lugar de capas totalmente conectadas hace que su peso sea menor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezamos las primeras 60 capas, reentrenamos las restantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:60]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agregamos una última capa al final para obtener las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation = \"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation = \"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cerramos el modelo y lo compilamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = Model(inputs = model.input, outputs = predictions)\n",
    "\n",
    "model_final.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = optimizers.SGD(lr = 0.0001, momentum = 0.9),\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generamos los conjuntos de datos de entrenamiento y validación\n",
    "\n",
    "Usamos data agumentation para contemplar rotaciones y otras variaciones. El concepto aquí es que no nos importa en qué parte de la imagen está la clase que queremos detectar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range = 0.3,\n",
    "    rotation_range = 30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range = 0.3,\n",
    "    rotation_range = 30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = \"binary\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size = (img_height, img_width),\n",
    "    class_mode = \"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, usamos *early stopping* y usamos checkpoints para ir guardando cada corrida. Al final guardamos los pesos de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"./models/demo_weights.hdf5\",\n",
    "    monitor = 'val_acc',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = False,\n",
    "    mode = 'auto',\n",
    "    period = 1)\n",
    "\n",
    "early = EarlyStopping(\n",
    "    monitor = 'val_acc',\n",
    "    min_delta = 0,\n",
    "    patience = 10,\n",
    "    verbose = 1,\n",
    "    mode = 'auto')\n",
    "\n",
    "model_final.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs, \n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size,\n",
    "    callbacks = [checkpoint, early])\n",
    "\n",
    "model_final.save('./models/demo.h5')\n",
    "print('Done with {} layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de esto hay que proceder a filtrar los resultados y segmentar las imágenes para terminar de detectar los objetos relevantes.\n",
    "\n",
    "Para segmentar las imágenes se puede usar la librería scikit-image u OpenCV.\n",
    "\n",
    "Algo relativamente sencillo que se puede hacer en este caso es:\n",
    "1. Remover los rectángulos pequeños y de baja probabilidad con un filtro de mediana\n",
    "2. Fusionar rectángulos que se solapan en un sólo polígono"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ([link al mapa](http://dymaxionlabs.com/ap-latam/en/map/?id=guatemala))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Guatemala](img/guatemala_map.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mar del Plata](img/mdq_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto lo hacemos con máquinas de [Google Cloud Platform](https://cloud.google.com). También usamos [Floydhub](https://floydhub.com) para las corridas de las redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros casos de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se suelen usar redes neuronales profundas también para poder clasificar usos del suelo. A partir de esto se pueden crear mapas a partir de imágenes como también detectar cambios a lo largo del tiempo para una región dada.\n",
    "\n",
    "Para esto se utilizan técnicas como las que comentamos anteriormente como sliding windows o filtros. Sin embargo, existen arquitecturas de redes que se pueden usar para realizar esa segmentación. Un ejemplo de esto es la red [U-Net](https://blog.deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/).\n",
    "\n",
    "Este tipo de red fue creada para trabajar con imágenes médicas pero también se usa para otros contextos.\n",
    "\n",
    "La arquitectura U-Net está implementada en Keras. Se puede acceder en https://github.com/zhixuhao/unet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![U-net paper](img/u_net_paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![U-Net](img/u_net_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![U-Net img2map](img/img2map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Dymaxion Labs usamos estas técnicas para detectar cambios a lo largo del tiempo. Tenemos un cliente con el que trabajamos esto para detectar parcelas sin permiso de obra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Change 1](img/seg_change1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Change 2](img/seg_change2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![meme](img/meme_ortiba.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
